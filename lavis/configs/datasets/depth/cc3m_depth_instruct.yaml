datasets:
  cc3m_depth_caption_instruct: # name of the dataset builder
    # data_dir: ${env.data_dir}/datasets
    data_type: videos # [images|videos|features]

    vis_processor:
      train:
        name: alpro_depth_train
        n_frms: 2
        image_size: 224
        min_scale: 0.9
        max_scale: 1.0
        full_video: True
      eval:
        name: alpro_depth_eval
        n_frms: 2
        image_size: 224
        min_scale: 0.9
        max_scale: 1.0
        full_video: True
    
    text_processor:
      train:
        name: blip_instruction
        task: caption
        modality: video
      eval:
        name: blip_caption

    build_info:
      # Be careful not to append minus sign (-) before split to avoid itemizing
      annotations:
        train:
          url: https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/msrvtt/cap_train.json
          storage: /15324359926/Multimodal/hm_repo/CC3M_Split/CC3M_train_50w_clear.json
        val:
          url: https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/msrvtt/qa_val.json
          storage: /15324359926/Multimodal/hm_repo/SUNRGBD/NYU-Depth-v2_val.json

      videos:
        storage: /15324359926/Multimodal/hm_repo/CC3M/all_train